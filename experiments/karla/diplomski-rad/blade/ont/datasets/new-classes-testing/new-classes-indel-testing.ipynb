{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import progressbar\n",
    "import pysam\n",
    "import pysamstats\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_file_path = '/home/diplomski-rad/blade/pb/escherichia-coli-NCTC86/reads-to-ref-sorted.bam'\n",
    "reference_fasta_path = '/home/data/pacific_biosciences/bacteria/escherichia/coli/escherichia_coli_reference.fasta'\n",
    "include_indels = True\n",
    "save_directory_path = './new-class-test'\n",
    "neighbourhood_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pileups: X and y together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_pileups(bam_file_path, reference_fasta_path, include_indels=True):\n",
    "    \"\"\"\n",
    "    Generate pileups from reads alignment to reference.\n",
    "\n",
    "    Pileups are generated for all contigs.\n",
    "\n",
    "    :param bam_file_path: path to .bam file containing alignments\n",
    "    :type bam_file_path: str\n",
    "    :param reference_fasta_path: path to .fasta file\n",
    "    :type reference_fasta_path: str\n",
    "    :param include_indels: flag which indicates whether to include indels in\n",
    "        pileups\n",
    "    :type include_indels: bool\n",
    "    :return: pileups (X)\n",
    "    :rtype: tuple of np.ndarray and list of str\n",
    "    \"\"\"\n",
    "    bam_file = pysam.AlignmentFile(bam_file_path)\n",
    "\n",
    "    if include_indels:\n",
    "        info_of_interest = ['A', 'C', 'G', 'T', 'insertions', 'deletions']\n",
    "        indel_positions = [4, 5]\n",
    "    else:\n",
    "        info_of_interest = ['A', 'C', 'G', 'T']\n",
    "        \n",
    "    # Last number in shape - 5 - is for letters other than A, C, G and T.\n",
    "    mapping = {'A': 0, 'a': 0, 'C': 1, 'c': 1, 'G': 2, 'g': 2, 'T': 3, 't': 3}\n",
    "    total_options = len(info_of_interest) + 1\n",
    "\n",
    "    pileups = [np.zeros(\n",
    "        (bam_file.get_reference_length(contig_name),\n",
    "         len(info_of_interest)\n",
    "         )) for contig_name in bam_file.references]\n",
    "    y_oh = [np.zeros(\n",
    "        (bam_file.get_reference_length(contig_name),\n",
    "         total_options\n",
    "         )) for contig_name in bam_file.references]\n",
    "\n",
    "    total_length = np.sum(\n",
    "        [bam_file.get_reference_length(contig_name) for contig_name in\n",
    "         bam_file.references])\n",
    "    progress_counter = 0\n",
    "    with progressbar.ProgressBar(max_value=total_length) as progress_bar:\n",
    "        for contig_id, contig_name in enumerate(bam_file.references):\n",
    "            for record in pysamstats.stat_variation(\n",
    "                    bam_file, chrom=contig_name, fafile=reference_fasta_path):\n",
    "                progress_bar.update(progress_counter)\n",
    "                progress_counter += 1\n",
    "                \n",
    "                curr_position = record['pos']\n",
    "                \n",
    "                # Parsing X.\n",
    "                # Note: Commented code is slower due to the fact that in python when using list comprehension\n",
    "                # new list is created everytime which is expensive. In other languages, like C++, you could\n",
    "                # create one array and use it every time.\n",
    "#                 curr_pileup = [record[info] for info in info_of_interest]\n",
    "#                 pileups[contig_id][curr_position] = curr_pileup\n",
    "                for i, info in enumerate(info_of_interest):\n",
    "                    pileups[contig_id][curr_position][i] += record[info]\n",
    "                \n",
    "                # Parsing y.\n",
    "#                 argmax_pileup = np.argmax(curr_pileup)\n",
    "#                 y_oh[contig_id][curr_position][argmax_pileup] = 1\n",
    "                if not include_indels:\n",
    "                    y_oh[contig_id][curr_position][mapping.get(record['ref'], -1)] = 1\n",
    "                else:\n",
    "                    pileup_argmax = np.argmax(pileups[contig_id][curr_position])\n",
    "                    if pileup_argmax in indel_positions:\n",
    "                        y_oh[contig_id][curr_position][pileup_argmax] = 1\n",
    "                    else:\n",
    "                        y_oh[contig_id][curr_position][mapping.get(record['ref'], -1)] = 1\n",
    "\n",
    "    return pileups, y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4641652 of 4641652) |##############| Elapsed Time: 0:00:54 Time:  0:00:54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b86e22eb86c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate_pileups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbam_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_fasta_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_indels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_indels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-7d2e5431f027>\u001b[0m in \u001b[0;36m_generate_pileups\u001b[0;34m(bam_file_path, reference_fasta_path, include_indels)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#                 pileups[contig_id][curr_position] = curr_pileup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_of_interest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mpileups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontig_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m# Parsing y.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y_oh = _generate_pileups(bam_file_path, reference_fasta_path, include_indels=include_indels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pileups: wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pileups(bam_file_path, reference_fasta_path, mode,\n",
    "                     save_directory_path=None, include_indels=True):\n",
    "    \"\"\"\n",
    "    Generates pileups from given alignment stored in bam file.\n",
    "\n",
    "    Mode must be one of 'training' or 'inference' string indicating pileups\n",
    "    generation mode. If 'training' is selected, pileups from different\n",
    "    contigs are all be concatenated. If 'inference' is selected, pileups from\n",
    "    different contig will be hold separate to enable to make consensus with\n",
    "    same number of contigs.\n",
    "\n",
    "    If save_directory_path is provided, generated pileups are stored in that\n",
    "    directory.\n",
    "\n",
    "    If include_indels is set to True, indels will also we included in\n",
    "    pileups. Otherwise, only nucleus bases will be in pileup (A, C, G and T).\n",
    "\n",
    "    :param bam_file_path: path to .bam file containing alignments\n",
    "    :type bam_file_path: str\n",
    "    :param reference_fasta_path: path to .fasta file\n",
    "    :type reference_fasta_path: str\n",
    "    :param mode: either 'training' or 'inference' string, representing the\n",
    "        mode for pileups generation\n",
    "    :type mode: str\n",
    "    :param save_directory_path: path to directory for storing pileups\n",
    "    :type save_directory_path: str\n",
    "    :param include_indels: flag which indicates whether to include indels in\n",
    "        pileups\n",
    "    :type include_indels: bool\n",
    "    :return: pileups (X) and matching nucleus bases from reference (y) with\n",
    "        concatenated contigs for inference mode, or separate contigs for\n",
    "        training mode; also, if save_directory_path is provided, list of\n",
    "        paths to saved files is returned in tuple (X, y_oh, X_save_paths,\n",
    "        y_save_paths). In both cases list of contig names is also returned.\n",
    "    :rtype tuple of np.ndarray and list of str or tuple of np.array and list of str\n",
    "    \"\"\"\n",
    "    _check_mode(mode)\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        if os.path.exists(save_directory_path):\n",
    "            raise ValueError('You must provide non-existing save output '\n",
    "                             'directory, {} given.'.format(save_directory_path))\n",
    "        else:\n",
    "            os.makedirs(save_directory_path)\n",
    "\n",
    "    print('##### Generate pileups from read alignments to reference. #####')\n",
    "    \n",
    "    X, y_oh = _generate_pileups(bam_file_path,\n",
    "                                           reference_fasta_path,\n",
    "                                           include_indels=include_indels)\n",
    "\n",
    "    total_pileups = len(X)\n",
    "    if mode == 'training': # training mode\n",
    "        X, y_oh = [np.concatenate(X, axis=0)], [np.concatenate(y_oh, axis=0)]\n",
    "        total_pileups = 1\n",
    "    else:  # inference mode\n",
    "        pass  # nothing to do\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        X_save_paths = [\n",
    "            os.path.join(\n",
    "                save_directory_path,\n",
    "                'pileups-X-{}{}.npy'.format(\n",
    "                    i, '-indels' if include_indels else ''))\n",
    "            for i in range(total_pileups)]\n",
    "        y_save_paths = [os.path.join(save_directory_path,\n",
    "                        'pileups-y-{}.npy'.format(i))\n",
    "                        for i in range(total_pileups)]\n",
    "        for X_save_path, y_save_path, Xi, yi in zip(\n",
    "                X_save_paths, y_save_paths, X, y_oh):\n",
    "            np.save(X_save_path, Xi)\n",
    "            np.save(y_save_path, yi)\n",
    "        return X, y_oh, X_save_paths, y_save_paths\n",
    "\n",
    "    return X, y_oh\n",
    "\n",
    "def _check_mode(mode):\n",
    "    \"\"\"\n",
    "    Checks if given mode is supported: 'training' or 'inference'.\n",
    "\n",
    "    :param mode: mode to check\n",
    "    :type mode: str\n",
    "    :raise ValueError: if selected mode is not supported\n",
    "    \"\"\"\n",
    "    modes = ['training', 'inference']\n",
    "    if mode not in modes:\n",
    "        raise ValueError('You must provide either \\'training\\' or '\n",
    "                         '\\'inference\\' mode, but \\'{}\\' given.'.format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1022 of 4641652) |                 | Elapsed Time: 0:00:00 ETA:   0:07:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Generate pileups from read alignments to reference. #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4641652 of 4641652) |##############| Elapsed Time: 0:07:19 Time:  0:07:19\n"
     ]
    }
   ],
   "source": [
    "X, y_oh, X_save_paths, y_save_paths = generate_pileups(\n",
    "    bam_file_path,\n",
    "    reference_fasta_path,\n",
    "    mode='training', \n",
    "    save_directory_path='./new-class-test',\n",
    "    include_indels=include_indels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset with neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_with_neighbourhood(X_paths, y_paths, neighbourhood_size,\n",
    "                                      mode, save_directory_path=None):\n",
    "    \"\"\"\n",
    "    Creates datasets by mixing all pileups with given neighbourhood_size.\n",
    "\n",
    "    Datasets are concatenated after extracting neighbourhood_size positions in\n",
    "    given datasets separately if 'training' mode is selected. If 'inference'\n",
    "    mode is selected, X_paths are assumed to be paths to different contigs of\n",
    "    same\n",
    "\n",
    "    Dataset at i-th position in X_paths should match given labels at i-th\n",
    "    position in y_paths.\n",
    "\n",
    "    If save_directory_path is provided, generated pileups are stored in that\n",
    "    directory.\n",
    "\n",
    "    :param X_paths: list of paths to X pileup dataset\n",
    "    :type X_paths: list of str\n",
    "    :param y_paths: list of paths to y pileup dataset\n",
    "    :type y_paths: list of str\n",
    "    :param neighbourhood_size: number of neighbours to use from one size (eg.\n",
    "        if you set this parameter to 3, it will take 3 neighbours from both\n",
    "        sides so total number of positions in one example will be 7 -\n",
    "        counting the middle position)\n",
    "    :type neighbourhood_size: int\n",
    "    :param mode: either 'training' or 'inference' string, representing the\n",
    "        mode for pileups generation\n",
    "    :type mode: str\n",
    "    :param save_directory_path: path to directory for storing dataset\n",
    "    :type save_directory_path: str\n",
    "    :return:\n",
    "    :rtype tuple of np.ndarray or tuple of np.array and list of str\n",
    "    \"\"\"\n",
    "    _check_mode(mode)\n",
    "\n",
    "    if not len(X_paths) == len(y_paths):\n",
    "        raise ValueError('Number of X_paths and y_paths should be the same!')\n",
    "\n",
    "    # If training mode is selected, all pileups will be concatenated.\n",
    "    total_pileups = 1 if mode == 'training' else len(X_paths)\n",
    "\n",
    "    X_save_paths, y_save_paths = None, None\n",
    "    if save_directory_path is not None:\n",
    "        X_save_paths, y_save_paths = _generate_save_paths(neighbourhood_size,\n",
    "                                                          save_directory_path,\n",
    "                                                          total_pileups)\n",
    "\n",
    "    X_neighbourhood_list, y_neighbourhood_list = list(), list()\n",
    "    for X_path, y_path in zip(X_paths, y_paths):\n",
    "        print('Parsing ', X_path, ' and ', y_path)\n",
    "\n",
    "        curr_X, curr_y = np.load(X_path), np.load(y_path)\n",
    "        # Removing last column which contains everything which was not 'A' nor\n",
    "        # 'C' nor 'G' nor 'T'.\n",
    "        curr_y = curr_y[:, :-1]\n",
    "        new_curr_X, new_curr_y = list(), list()\n",
    "        empty_rows = _calc_empty_rows(curr_X)\n",
    "\n",
    "        print('Creating dataset with neighbourhood ...')\n",
    "        with progressbar.ProgressBar(max_value=curr_X.shape[0]) as progress_bar:\n",
    "            # TODO(ajuric): Check if this can be speed up.\n",
    "            for i in range(curr_X.shape[0]):\n",
    "                progress_bar.update(i)\n",
    "                if empty_rows[i] == 1:\n",
    "                    continue  # current row is empty row\n",
    "                if i < neighbourhood_size or \\\n",
    "                   i >= curr_X.shape[0] - neighbourhood_size:\n",
    "                    # Current position is not suitable to build an example.\n",
    "                    continue\n",
    "                zeros_to_left = np.sum(empty_rows[i - neighbourhood_size:i])\n",
    "                zeros_to_right = np.sum(\n",
    "                    empty_rows[i + 1:i + neighbourhood_size + 1])\n",
    "                if zeros_to_left == 0 and zeros_to_right == 0:\n",
    "                    new_curr_X.append(\n",
    "                        curr_X[\n",
    "                            i - neighbourhood_size:\n",
    "                            i + neighbourhood_size + 1])\n",
    "                    new_curr_y.append(curr_y[i])\n",
    "\n",
    "        X_neighbourhood_list.append(np.array(new_curr_X))\n",
    "        y_neighbourhood_list.append(np.array(new_curr_y))\n",
    "\n",
    "    if mode == 'training':\n",
    "        X_neighbourhood_list = [np.concatenate(X_neighbourhood_list, axis=0)]\n",
    "        y_neighbourhood_list = [np.concatenate(y_neighbourhood_list, axis=0)]\n",
    "    else:  # inference mode\n",
    "        pass  # nothing to do\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        for X_save_path, y_save_path, Xi, yi in zip(\n",
    "            X_save_paths, y_save_paths, X_neighbourhood_list,\n",
    "                y_neighbourhood_list):\n",
    "            np.save(X_save_path, Xi)\n",
    "            np.save(y_save_path, yi)\n",
    "        return X_neighbourhood_list, \\\n",
    "               y_neighbourhood_list, \\\n",
    "               X_save_paths, \\\n",
    "               y_save_paths\n",
    "\n",
    "    return X_neighbourhood_list, y_neighbourhood_list\n",
    "\n",
    "\n",
    "def _calc_empty_rows(X):\n",
    "    \"\"\"\n",
    "    Calculates which rows in X are empty rows (i.e. all numbers in that row\n",
    "    are equal to 0).\n",
    "\n",
    "    :param X: 2-D data\n",
    "    :type X: np.ndarray\n",
    "    :return: 1-D array with 1s on positions which correspond to empty rows.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    empty_row = np.zeros((1, X.shape[1]))  # size is second axis of X\n",
    "    empty_rows = [int(v) for v in np.all(empty_row == X, axis=1)]\n",
    "    return empty_rows\n",
    "\n",
    "\n",
    "def _generate_save_paths(neighbourhood_size, save_directory_path,\n",
    "                         total_pileups):\n",
    "    \"\"\"\n",
    "    Generates a list of save paths for dataset X and y.\n",
    "\n",
    "    :param neighbourhood_size: number of neighbours to use from one size (eg.\n",
    "        if you set this parameter to 3, it will take 3 neighbours from both\n",
    "        sides so total number of positions in one example will be 7 -\n",
    "        counting the middle position)\n",
    "    :type neighbourhood_size: int\n",
    "    :param save_directory_path: path to directory for storing dataset\n",
    "    :type save_directory_path: str\n",
    "    :param total_pileups: total number of pileups to be generated a the end;\n",
    "        determines the number of save paths to be generated\n",
    "    :type total_pileups: int\n",
    "    :return: tuple of list of str\n",
    "    \"\"\"\n",
    "    # Creating save file paths.\n",
    "    X_save_paths = [os.path.join(\n",
    "        save_directory_path,\n",
    "        'X-pileups-n{}-{}.npy'.format(neighbourhood_size, i))\n",
    "                    for i in range(total_pileups)]\n",
    "    y_save_paths = [os.path.join(\n",
    "        save_directory_path,\n",
    "        'y-pileups-n{}-{}.npy'.format(neighbourhood_size, i))\n",
    "                    for i in range(total_pileups)]\n",
    "\n",
    "    for X_save_path, y_save_path in zip(X_save_paths, y_save_paths):\n",
    "        if os.path.exists(X_save_path) or os.path.exists(y_save_path):\n",
    "            raise ValueError('Pileups already exists in given save '\n",
    "                             'directory. Either provide other save '\n",
    "                             'directory or empty this one.')\n",
    "    return X_save_paths, y_save_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing  ./new-class-test/pileups-X-0-indels.npy  and  ./new-class-test/pileups-y-0.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (4240 of 4641652) |                 | Elapsed Time: 0:00:00 ETA:   0:03:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with neighbourhood ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4641652 of 4641652) |##############| Elapsed Time: 0:03:37 Time:  0:03:37\n"
     ]
    }
   ],
   "source": [
    "X_neighbourhood_list, y_neighbourhood_list, X_save_paths, y_save_paths = create_dataset_with_neighbourhood(\n",
    "    X_save_paths, \n",
    "    y_save_paths, \n",
    "    neighbourhood_size=neighbourhood_size, \n",
    "    mode='training', \n",
    "    save_directory_path=save_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape dataset for conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_and_reshape_for_conv(X_paths, y_paths, validation_size=None,\n",
    "                                      save_directory_path=None):\n",
    "    \"\"\"\n",
    "    Reads X and y from given paths and reshapes them for applying in\n",
    "    convolutional networks.\n",
    "\n",
    "    Reshaping is done by splitting different letters in separate channels,\n",
    "    eg. letter 'A' has it's own channel, letter 'C' has it's own channel, etc.\n",
    "\n",
    "    :param path_X: list of paths to X data\n",
    "    :type path_X: list of str\n",
    "    :param path_y: list of paths to y data\n",
    "    :type path_y: list of str\n",
    "    :param validation_size: specifies percentage of dataset used for validation\n",
    "    :type validation_size: float\n",
    "    :return: If validation_size is None, returns just X and y reshaped. If\n",
    "    validation_size is float, returns a tuple in following order: (X, y,\n",
    "    X_train, X_validate, y_train, y_validate).\n",
    "    :rtype: tuple of np.ndarray\n",
    "    \"\"\"\n",
    "    if not len(X_paths) == len(y_paths):\n",
    "        raise ValueError('Number of X_paths and y_paths must be the same!')\n",
    "\n",
    "    if validation_size is not None:\n",
    "        if validation_size < 0 or validation_size > 1.0:\n",
    "            raise ValueError('Validation size must be float from [0, 1], but {}'\n",
    "                             ' given.'.format(validation_size))\n",
    "        if not len(X_paths) == 1:\n",
    "            raise ValueError(\n",
    "                'Validation size can only be provided if there is only one X_path and y_path.')\n",
    "\n",
    "    X_save_paths, y_save_paths = None, None\n",
    "    if save_directory_path is not None:\n",
    "        pass\n",
    "\n",
    "    X_list, y_list = list(), list()\n",
    "    for X_path, y_path in zip(X_paths, y_paths):\n",
    "        X, y = np.load(X_path), np.load(y_path)\n",
    "        print('X shape before reshaping:', X.shape)\n",
    "        print('y shape before reshaping:', y.shape)\n",
    "\n",
    "        new_X = list()\n",
    "        neighbourhood_size = X[0].shape[0]\n",
    "        # Number of columns is equal to the number of letters in dataset (A, C,\n",
    "        # G, T, I, D, ...).\n",
    "        num_columns = X[0].shape[1]\n",
    "        num_data = X.shape[0]\n",
    "        with progressbar.ProgressBar(max_value=num_data) as progress_bar:\n",
    "            for i, xi in enumerate(X):\n",
    "                new_xi = np.dstack(\n",
    "                    [xi[:, col_index].reshape(neighbourhood_size, 1)\n",
    "                     for col_index in range(num_columns)]\n",
    "                )\n",
    "                new_X.append(new_xi)\n",
    "                progress_bar.update(i)\n",
    "\n",
    "        new_X = np.array(new_X)\n",
    "        X = new_X\n",
    "        print('X shape after reshaping:', X.shape)\n",
    "        print('y shape after reshaping:', y.shape)\n",
    "\n",
    "        X_list.append(X), y_list.append(y)\n",
    "\n",
    "    if validation_size is None:\n",
    "        return X_list, y_list\n",
    "    else:\n",
    "        # There is only one X and y (because, all datasets are concatenated for training).\n",
    "        X, y = X_list[0], y_list[0]\n",
    "        print('Splitting to train and validation set.')\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "            X, y, test_size=validation_size)\n",
    "        print('X_train shape:', X_train.shape)\n",
    "        print('X_validate shape:', X_validate.shape)\n",
    "        print('y_train:', y_train.shape)\n",
    "        print('y_validate:', y_validate.shape)\n",
    "        return X, y, X_train, X_validate, y_train, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (2391 of 4495756) |                 | Elapsed Time: 0:00:00 ETA:   0:03:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before reshaping: (4495756, 7, 6)\n",
      "y shape before reshaping: (4495756, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4495756 of 4495756) |##############| Elapsed Time: 0:03:01 Time:  0:03:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after reshaping: (4495756, 7, 1, 6)\n",
      "y shape after reshaping: (4495756, 6)\n",
      "Splitting to train and validation set.\n",
      "X_train shape: (4046180, 7, 1, 6)\n",
      "X_validate shape: (449576, 7, 1, 6)\n",
      "y_train: (4046180, 6)\n",
      "y_validate: (449576, 6)\n"
     ]
    }
   ],
   "source": [
    "X, y, X_train, X_validate, y_train, y_validate = read_dataset_and_reshape_for_conv(\n",
    "    X_save_paths,\n",
    "    y_save_paths,\n",
    "    validation_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "80439.0\n",
      "0.017892207673192228\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 4:])\n",
    "ca\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "1083064.0\n",
      "0.2409080919871986\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 0:1])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "1127378.0\n",
      "0.25076494364907703\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 1:2])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "1124101.0\n",
      "0.2500360339840507\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 2:3])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "1080774.0\n",
      "0.2403987227064814\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 3:4])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "1823.0\n",
      "0.0004054935365709349\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 4:5])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4495756, 6)\n",
      "78616.0\n",
      "0.017486714136621295\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "indel_classes = np.sum(y[:, 5:6])\n",
    "\n",
    "print(indel_classes)\n",
    "print(indel_classes / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
