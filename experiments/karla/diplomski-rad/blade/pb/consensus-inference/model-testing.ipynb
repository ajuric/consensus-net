{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, SeparableConv1D\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/home/diplomski-rad/blade/pb/datasets/n20-racon-hax/dataset-n20-X-train.npy')\n",
    "X_valid = np.load('/home/diplomski-rad/blade/pb/datasets/n20-racon-hax/dataset-n20-X-validate.npy')\n",
    "y_train = np.load('/home/diplomski-rad/blade/pb/datasets/n20-racon-hax/dataset-n20-y-train.npy')\n",
    "y_valid = np.load('/home/diplomski-rad/blade/pb/datasets/n20-racon-hax/dataset-n20-y-validate.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 20, 40)            4840      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 10,286\n",
      "Trainable params: 10,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/10\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.4116 - acc: 0.9701 - val_loss: 0.0940 - val_acc: 0.9872\n",
      "Epoch 2/10\n",
      "27981697/27981697 [==============================] - 241s 9us/step - loss: 0.0801 - acc: 0.9878 - val_loss: 0.0728 - val_acc: 0.9878\n",
      "Epoch 3/10\n",
      "27981697/27981697 [==============================] - 250s 9us/step - loss: 0.0702 - acc: 0.9879 - val_loss: 0.0710 - val_acc: 0.9879\n",
      "Epoch 4/10\n",
      "27981697/27981697 [==============================] - 238s 9us/step - loss: 0.0671 - acc: 0.9881 - val_loss: 0.0678 - val_acc: 0.9876\n",
      "Epoch 5/10\n",
      "27981697/27981697 [==============================] - 253s 9us/step - loss: 0.0656 - acc: 0.9882 - val_loss: 0.0651 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "27981697/27981697 [==============================] - 241s 9us/step - loss: 0.0647 - acc: 0.9882 - val_loss: 0.0649 - val_acc: 0.9879\n",
      "Epoch 7/10\n",
      "27981697/27981697 [==============================] - 250s 9us/step - loss: 0.0641 - acc: 0.9883 - val_loss: 0.0643 - val_acc: 0.9884\n",
      "Epoch 8/10\n",
      "27981697/27981697 [==============================] - 244s 9us/step - loss: 0.0636 - acc: 0.9883 - val_loss: 0.0632 - val_acc: 0.9885\n",
      "Epoch 9/10\n",
      "27981697/27981697 [==============================] - 251s 9us/step - loss: 0.0632 - acc: 0.9883 - val_loss: 0.0635 - val_acc: 0.9880\n",
      "Epoch 10/10\n",
      "27981697/27981697 [==============================] - 237s 8us/step - loss: 0.0628 - acc: 0.9883 - val_loss: 0.0628 - val_acc: 0.9884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10846be908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 20, 40)            4840      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 10,286\n",
      "Trainable params: 10,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/10\n",
      "27981697/27981697 [==============================] - 277s 10us/step - loss: 0.6705 - acc: 0.9713 - val_loss: 0.1482 - val_acc: 0.9860\n",
      "Epoch 2/10\n",
      "27981697/27981697 [==============================] - 276s 10us/step - loss: 0.1372 - acc: 0.9862 - val_loss: 0.1287 - val_acc: 0.9863\n",
      "Epoch 3/10\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.1232 - acc: 0.9864 - val_loss: 0.1193 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.1164 - acc: 0.9865 - val_loss: 0.1138 - val_acc: 0.9867\n",
      "Epoch 5/10\n",
      "27981697/27981697 [==============================] - 275s 10us/step - loss: 0.1126 - acc: 0.9866 - val_loss: 0.1134 - val_acc: 0.9867\n",
      "Epoch 6/10\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.1099 - acc: 0.9867 - val_loss: 0.1090 - val_acc: 0.9868\n",
      "Epoch 7/10\n",
      "27981697/27981697 [==============================] - 274s 10us/step - loss: 0.1078 - acc: 0.9867 - val_loss: 0.1085 - val_acc: 0.9866\n",
      "Epoch 8/10\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.1061 - acc: 0.9868 - val_loss: 0.1053 - val_acc: 0.9867\n",
      "Epoch 9/10\n",
      "27981697/27981697 [==============================] - 271s 10us/step - loss: 0.1046 - acc: 0.9868 - val_loss: 0.1040 - val_acc: 0.9868\n",
      "Epoch 10/10\n",
      "27981697/27981697 [==============================] - 272s 10us/step - loss: 0.1036 - acc: 0.9869 - val_loss: 0.1030 - val_acc: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f113c056ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 20, 40)            4840      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 10,286\n",
      "Trainable params: 10,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/10\n",
      "27981697/27981697 [==============================] - 293s 10us/step - loss: 0.7210 - acc: 0.9716 - val_loss: 0.1471 - val_acc: 0.9865\n",
      "Epoch 2/10\n",
      "27981697/27981697 [==============================] - 285s 10us/step - loss: 0.1358 - acc: 0.9865 - val_loss: 0.1286 - val_acc: 0.9864\n",
      "Epoch 3/10\n",
      "27981697/27981697 [==============================] - 284s 10us/step - loss: 0.1246 - acc: 0.9866 - val_loss: 0.1210 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "27981697/27981697 [==============================] - 285s 10us/step - loss: 0.1189 - acc: 0.9866 - val_loss: 0.1187 - val_acc: 0.9866\n",
      "Epoch 5/10\n",
      "27981697/27981697 [==============================] - 282s 10us/step - loss: 0.1154 - acc: 0.9867 - val_loss: 0.1146 - val_acc: 0.9863\n",
      "Epoch 6/10\n",
      "27981697/27981697 [==============================] - 288s 10us/step - loss: 0.1131 - acc: 0.9867 - val_loss: 0.1118 - val_acc: 0.9868\n",
      "Epoch 7/10\n",
      "27981697/27981697 [==============================] - 281s 10us/step - loss: 0.1111 - acc: 0.9867 - val_loss: 0.1106 - val_acc: 0.9869\n",
      "Epoch 8/10\n",
      "27981697/27981697 [==============================] - 285s 10us/step - loss: 0.1096 - acc: 0.9867 - val_loss: 0.1088 - val_acc: 0.9868\n",
      "Epoch 9/10\n",
      "27981697/27981697 [==============================] - 286s 10us/step - loss: 0.1084 - acc: 0.9868 - val_loss: 0.1078 - val_acc: 0.9869\n",
      "Epoch 10/10\n",
      "27981697/27981697 [==============================] - 289s 10us/step - loss: 0.1073 - acc: 0.9868 - val_loss: 0.1061 - val_acc: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f112c2c31d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1_l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1_l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise separable convolution - first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_1 (Separabl (None, 41, 40)            255       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 20, 40)            4840      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 9,901\n",
      "Trainable params: 9,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 282s 10us/step - loss: 0.3144 - acc: 0.9583 - val_loss: 0.0936 - val_acc: 0.9860\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 271s 10us/step - loss: 0.0800 - acc: 0.9868 - val_loss: 0.0790 - val_acc: 0.9870\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 275s 10us/step - loss: 0.0720 - acc: 0.9872 - val_loss: 0.0768 - val_acc: 0.9868\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 271s 10us/step - loss: 0.0679 - acc: 0.9876 - val_loss: 0.0666 - val_acc: 0.9877\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 274s 10us/step - loss: 0.0652 - acc: 0.9879 - val_loss: 0.0661 - val_acc: 0.9874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f047c7cea58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise separable convolution - second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_2 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 7,206\n",
      "Trainable params: 7,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 240s 9us/step - loss: 0.1169 - acc: 0.9778 - val_loss: 0.0675 - val_acc: 0.9878\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 236s 8us/step - loss: 0.0635 - acc: 0.9881 - val_loss: 0.0627 - val_acc: 0.9880\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 236s 8us/step - loss: 0.0605 - acc: 0.9884 - val_loss: 0.0617 - val_acc: 0.9882\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 236s 8us/step - loss: 0.0591 - acc: 0.9886 - val_loss: 0.0585 - val_acc: 0.9890\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 231s 8us/step - loss: 0.0581 - acc: 0.9888 - val_loss: 0.0577 - val_acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04347d4358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depthwise separable convolution - all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_3 (Separabl (None, 41, 40)            255       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_4 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 6,821\n",
      "Trainable params: 6,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 264s 9us/step - loss: 0.1197 - acc: 0.9716 - val_loss: 0.0735 - val_acc: 0.9871\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 262s 9us/step - loss: 0.0685 - acc: 0.9874 - val_loss: 0.0657 - val_acc: 0.9874\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.0631 - acc: 0.9878 - val_loss: 0.0623 - val_acc: 0.9880\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 262s 9us/step - loss: 0.0612 - acc: 0.9880 - val_loss: 0.0607 - val_acc: 0.9881\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 263s 9us/step - loss: 0.0602 - acc: 0.9882 - val_loss: 0.0599 - val_acc: 0.9881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f041bab2438>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger kernel - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 41, 40)            1040      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_5 (Separabl (None, 20, 40)            1840      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 7,686\n",
      "Trainable params: 7,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 242s 9us/step - loss: 0.1425 - acc: 0.9731 - val_loss: 0.0746 - val_acc: 0.9878\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 238s 9us/step - loss: 0.0690 - acc: 0.9879 - val_loss: 0.0652 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.0633 - acc: 0.9881 - val_loss: 0.0623 - val_acc: 0.9883\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 236s 8us/step - loss: 0.0612 - acc: 0.9884 - val_loss: 0.0618 - val_acc: 0.9880\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 237s 8us/step - loss: 0.0601 - acc: 0.9885 - val_loss: 0.0604 - val_acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f041aaa3320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=5, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=5, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger kernel - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 41, 40)            2240      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_7 (Separabl (None, 20, 40)            2080      \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 9,126\n",
      "Trainable params: 9,126\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 249s 9us/step - loss: 0.1210 - acc: 0.9775 - val_loss: 0.0753 - val_acc: 0.9878\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 247s 9us/step - loss: 0.0704 - acc: 0.9878 - val_loss: 0.0686 - val_acc: 0.9880\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 243s 9us/step - loss: 0.0650 - acc: 0.9881 - val_loss: 0.0649 - val_acc: 0.9881\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 244s 9us/step - loss: 0.0630 - acc: 0.9884 - val_loss: 0.0636 - val_acc: 0.9884\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 245s 9us/step - loss: 0.0621 - acc: 0.9885 - val_loss: 0.0633 - val_acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0419f062e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=11, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=11, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "\n",
    "flatten = Flatten()(conv_2)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_8 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 7,366\n",
      "Trainable params: 7,286\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 262s 9us/step - loss: 0.1015 - acc: 0.9794 - val_loss: 0.0608 - val_acc: 0.9884\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 257s 9us/step - loss: 0.0590 - acc: 0.9886 - val_loss: 0.0589 - val_acc: 0.9889\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 258s 9us/step - loss: 0.0577 - acc: 0.9889 - val_loss: 0.0575 - val_acc: 0.9890\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 256s 9us/step - loss: 0.0569 - acc: 0.9890 - val_loss: 0.0568 - val_acc: 0.9891\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 254s 9us/step - loss: 0.0563 - acc: 0.9892 - val_loss: 0.0561 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11550fb240>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dense block + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_1 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                16020     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 18,706\n",
      "Trainable params: 18,626\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 259s 9us/step - loss: 0.1250 - acc: 0.9820 - val_loss: 0.0688 - val_acc: 0.9880\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 254s 9us/step - loss: 0.0660 - acc: 0.9885 - val_loss: 0.0656 - val_acc: 0.9883\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 241s 9us/step - loss: 0.0638 - acc: 0.9887 - val_loss: 0.0635 - val_acc: 0.9889\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 246s 9us/step - loss: 0.0623 - acc: 0.9889 - val_loss: 0.0626 - val_acc: 0.9890\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 256s 9us/step - loss: 0.0610 - acc: 0.9892 - val_loss: 0.0619 - val_acc: 0.9890\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 262s 9us/step - loss: 0.0599 - acc: 0.9893 - val_loss: 0.0613 - val_acc: 0.9892\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.0591 - acc: 0.9893 - val_loss: 0.0601 - val_acc: 0.9891\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 264s 9us/step - loss: 0.0586 - acc: 0.9894 - val_loss: 0.0596 - val_acc: 0.9893\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 259s 9us/step - loss: 0.0583 - acc: 0.9894 - val_loss: 0.0590 - val_acc: 0.9893\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 260s 9us/step - loss: 0.0581 - acc: 0.9895 - val_loss: 0.0613 - val_acc: 0.9889\n",
      "Epoch 11/15\n",
      "27981697/27981697 [==============================] - 259s 9us/step - loss: 0.0579 - acc: 0.9895 - val_loss: 0.0577 - val_acc: 0.9896\n",
      "Epoch 12/15\n",
      "27981697/27981697 [==============================] - 259s 9us/step - loss: 0.0578 - acc: 0.9895 - val_loss: 0.0622 - val_acc: 0.9889\n",
      "Epoch 13/15\n",
      "27981697/27981697 [==============================] - 255s 9us/step - loss: 0.0578 - acc: 0.9895 - val_loss: 0.0584 - val_acc: 0.9895\n",
      "Epoch 14/15\n",
      "27981697/27981697 [==============================] - 255s 9us/step - loss: 0.0577 - acc: 0.9895 - val_loss: 0.0585 - val_acc: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e4bf2e80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(20, kernel_regularizer=l2(0.01))(flatten)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(dense_1)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dense block + l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_2 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                16020     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 18,706\n",
      "Trainable params: 18,626\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.3056 - acc: 0.9793 - val_loss: 0.1086 - val_acc: 0.9878\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 234s 8us/step - loss: 0.1004 - acc: 0.9880 - val_loss: 0.0968 - val_acc: 0.9883\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 230s 8us/step - loss: 0.0944 - acc: 0.9881 - val_loss: 0.0961 - val_acc: 0.9882\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 228s 8us/step - loss: 0.0919 - acc: 0.9881 - val_loss: 0.0942 - val_acc: 0.9879\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 230s 8us/step - loss: 0.0905 - acc: 0.9883 - val_loss: 0.0911 - val_acc: 0.9884\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 225s 8us/step - loss: 0.0897 - acc: 0.9884 - val_loss: 0.0925 - val_acc: 0.9883\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 231s 8us/step - loss: 0.0894 - acc: 0.9885 - val_loss: 0.0891 - val_acc: 0.9889\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.0891 - acc: 0.9886 - val_loss: 0.0907 - val_acc: 0.9885\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 233s 8us/step - loss: 0.0890 - acc: 0.9887 - val_loss: 0.0903 - val_acc: 0.9887\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 228s 8us/step - loss: 0.0888 - acc: 0.9888 - val_loss: 0.0894 - val_acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e4a636d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(20, kernel_regularizer=l1_l2(0.01))(flatten)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(dense_1)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger Dense block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_3 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                40050     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 42,916\n",
      "Trainable params: 42,836\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 249s 9us/step - loss: 0.1236 - acc: 0.9837 - val_loss: 0.0703 - val_acc: 0.9880\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 245s 9us/step - loss: 0.0676 - acc: 0.9884 - val_loss: 0.0676 - val_acc: 0.9883\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 241s 9us/step - loss: 0.0646 - acc: 0.9887 - val_loss: 0.0636 - val_acc: 0.9890\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.0623 - acc: 0.9890 - val_loss: 0.0622 - val_acc: 0.9892\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 229s 8us/step - loss: 0.0610 - acc: 0.9891 - val_loss: 0.0612 - val_acc: 0.9892\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.0603 - acc: 0.9892 - val_loss: 0.0604 - val_acc: 0.9892\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 231s 8us/step - loss: 0.0598 - acc: 0.9892 - val_loss: 0.0601 - val_acc: 0.9893\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 239s 9us/step - loss: 0.0594 - acc: 0.9893 - val_loss: 0.0609 - val_acc: 0.9893\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 241s 9us/step - loss: 0.0590 - acc: 0.9893 - val_loss: 0.0619 - val_acc: 0.9891\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 270s 10us/step - loss: 0.0587 - acc: 0.9894 - val_loss: 0.0597 - val_acc: 0.9893\n",
      "Epoch 11/15\n",
      "27981697/27981697 [==============================] - 269s 10us/step - loss: 0.0585 - acc: 0.9894 - val_loss: 0.0585 - val_acc: 0.9895\n",
      "Epoch 12/15\n",
      "27981697/27981697 [==============================] - 279s 10us/step - loss: 0.0583 - acc: 0.9894 - val_loss: 0.0585 - val_acc: 0.9895\n",
      "Epoch 13/15\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.0582 - acc: 0.9895 - val_loss: 0.0593 - val_acc: 0.9892\n",
      "Epoch 14/15\n",
      "27981697/27981697 [==============================] - 280s 10us/step - loss: 0.0581 - acc: 0.9895 - val_loss: 0.0603 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e4101940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(50, kernel_regularizer=l2(0.01))(flatten)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(dense_1)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additonal Dense block with BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_4 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                16020     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 18,786\n",
      "Trainable params: 18,666\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 280s 10us/step - loss: 0.1164 - acc: 0.9797 - val_loss: 0.0649 - val_acc: 0.9885\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 258s 9us/step - loss: 0.0640 - acc: 0.9887 - val_loss: 0.0644 - val_acc: 0.9887\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 260s 9us/step - loss: 0.0625 - acc: 0.9890 - val_loss: 0.0629 - val_acc: 0.9889\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 250s 9us/step - loss: 0.0614 - acc: 0.9891 - val_loss: 0.0614 - val_acc: 0.9890\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 263s 9us/step - loss: 0.0606 - acc: 0.9892 - val_loss: 0.0608 - val_acc: 0.9891\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 253s 9us/step - loss: 0.0601 - acc: 0.9893 - val_loss: 0.0604 - val_acc: 0.9891\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 265s 9us/step - loss: 0.0597 - acc: 0.9893 - val_loss: 0.0600 - val_acc: 0.9894\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 246s 9us/step - loss: 0.0594 - acc: 0.9893 - val_loss: 0.0603 - val_acc: 0.9893\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 256s 9us/step - loss: 0.0592 - acc: 0.9894 - val_loss: 0.0592 - val_acc: 0.9894\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 249s 9us/step - loss: 0.0590 - acc: 0.9894 - val_loss: 0.0599 - val_acc: 0.9893\n",
      "Epoch 11/15\n",
      "27981697/27981697 [==============================] - 254s 9us/step - loss: 0.0589 - acc: 0.9894 - val_loss: 0.0602 - val_acc: 0.9892\n",
      "Epoch 12/15\n",
      "27981697/27981697 [==============================] - 255s 9us/step - loss: 0.0589 - acc: 0.9894 - val_loss: 0.0597 - val_acc: 0.9895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e39c4a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(20, kernel_regularizer=l2(0.01))(flatten)\n",
    "bn_2 = BatchNormalization()(dense_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(bn_2)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dense block with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_5 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                16020     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 18,706\n",
      "Trainable params: 18,626\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 260s 9us/step - loss: 0.1290 - acc: 0.9813 - val_loss: 0.0696 - val_acc: 0.9880\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 255s 9us/step - loss: 0.0719 - acc: 0.9884 - val_loss: 0.0668 - val_acc: 0.9883\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 274s 10us/step - loss: 0.0698 - acc: 0.9886 - val_loss: 0.0651 - val_acc: 0.9888\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 258s 9us/step - loss: 0.0684 - acc: 0.9889 - val_loss: 0.0643 - val_acc: 0.9890\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 273s 10us/step - loss: 0.0674 - acc: 0.9890 - val_loss: 0.0638 - val_acc: 0.9890\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 254s 9us/step - loss: 0.0669 - acc: 0.9890 - val_loss: 0.0624 - val_acc: 0.9892\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 259s 9us/step - loss: 0.0664 - acc: 0.9891 - val_loss: 0.0636 - val_acc: 0.9893\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 258s 9us/step - loss: 0.0662 - acc: 0.9892 - val_loss: 0.0625 - val_acc: 0.9893\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 265s 9us/step - loss: 0.0661 - acc: 0.9892 - val_loss: 0.0622 - val_acc: 0.9893\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 255s 9us/step - loss: 0.0660 - acc: 0.9892 - val_loss: 0.0627 - val_acc: 0.9892\n",
      "Epoch 11/15\n",
      "27981697/27981697 [==============================] - 266s 10us/step - loss: 0.0659 - acc: 0.9892 - val_loss: 0.0628 - val_acc: 0.9891\n",
      "Epoch 12/15\n",
      "27981697/27981697 [==============================] - 250s 9us/step - loss: 0.0658 - acc: 0.9892 - val_loss: 0.0618 - val_acc: 0.9893\n",
      "Epoch 13/15\n",
      "27981697/27981697 [==============================] - 265s 9us/step - loss: 0.0657 - acc: 0.9892 - val_loss: 0.0618 - val_acc: 0.9893\n",
      "Epoch 14/15\n",
      "27981697/27981697 [==============================] - 256s 9us/step - loss: 0.0657 - acc: 0.9892 - val_loss: 0.0621 - val_acc: 0.9892\n",
      "Epoch 15/15\n",
      "27981697/27981697 [==============================] - 269s 10us/step - loss: 0.0656 - acc: 0.9892 - val_loss: 0.0621 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e2b556a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(20, kernel_regularizer=l2(0.01))(flatten)\n",
    "drop = Dropout(0.25)(dense_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(drop)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dense block with BN + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_6 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                16020     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 18,786\n",
      "Trainable params: 18,666\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/15\n",
      "27981697/27981697 [==============================] - 265s 9us/step - loss: 0.1221 - acc: 0.9801 - val_loss: 0.0704 - val_acc: 0.9876\n",
      "Epoch 2/15\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.0700 - acc: 0.9886 - val_loss: 0.0657 - val_acc: 0.9888\n",
      "Epoch 3/15\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.0684 - acc: 0.9888 - val_loss: 0.0657 - val_acc: 0.9890\n",
      "Epoch 4/15\n",
      "27981697/27981697 [==============================] - 261s 9us/step - loss: 0.0675 - acc: 0.9889 - val_loss: 0.0632 - val_acc: 0.9892\n",
      "Epoch 5/15\n",
      "27981697/27981697 [==============================] - 257s 9us/step - loss: 0.0668 - acc: 0.9890 - val_loss: 0.0622 - val_acc: 0.9893\n",
      "Epoch 6/15\n",
      "27981697/27981697 [==============================] - 257s 9us/step - loss: 0.0663 - acc: 0.9891 - val_loss: 0.0639 - val_acc: 0.9893\n",
      "Epoch 7/15\n",
      "27981697/27981697 [==============================] - 265s 9us/step - loss: 0.0662 - acc: 0.9891 - val_loss: 0.0621 - val_acc: 0.9894\n",
      "Epoch 8/15\n",
      "27981697/27981697 [==============================] - 264s 9us/step - loss: 0.0660 - acc: 0.9892 - val_loss: 0.0624 - val_acc: 0.9894\n",
      "Epoch 9/15\n",
      "27981697/27981697 [==============================] - 267s 10us/step - loss: 0.0659 - acc: 0.9892 - val_loss: 0.0625 - val_acc: 0.9894\n",
      "Epoch 10/15\n",
      "27981697/27981697 [==============================] - 277s 10us/step - loss: 0.0659 - acc: 0.9892 - val_loss: 0.0631 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e2d29128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "dense_1 = Dense(20, kernel_regularizer=l2(0.01))(flatten)\n",
    "drop = Dropout(0.25)(dense_1)\n",
    "bn_2 = BatchNormalization()(drop)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(bn_2)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (27981697 of 27981697) |############| Elapsed Time: 0:01:44 Time:  0:01:44\n",
      "100% (3109078 of 3109078) |##############| Elapsed Time: 0:00:11 Time:  0:00:11\n"
     ]
    }
   ],
   "source": [
    "X_train_T = list()\n",
    "\n",
    "with ProgressBar(max_value=X_train.shape[0]) as progress_bar:\n",
    "    for i, xi in enumerate(X_train):\n",
    "        progress_bar.update(i)\n",
    "        X_train_T.append(xi.T)\n",
    "X_train_T = np.array(X_train_T)\n",
    "\n",
    "X_valid_T = list()\n",
    "with ProgressBar(max_value=X_valid.shape[0]) as progress_bar:\n",
    "    for i, xi in enumerate(X_valid):\n",
    "        progress_bar.update(i)\n",
    "        X_valid_T.append(xi.T)\n",
    "X_valid_T = np.array(X_valid_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed data - smaller kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 41)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 5, 40)             4960      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 2, 40)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_4 (Separabl (None, 2, 40)             1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 40)             160       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 7,366\n",
      "Trainable params: 7,286\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 175s 6us/step - loss: 0.1103 - acc: 0.9778 - val_loss: 0.0603 - val_acc: 0.9888\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 167s 6us/step - loss: 0.0589 - acc: 0.9889 - val_loss: 0.0582 - val_acc: 0.9891\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 163s 6us/step - loss: 0.0580 - acc: 0.9890 - val_loss: 0.0582 - val_acc: 0.9890\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 163s 6us/step - loss: 0.0577 - acc: 0.9891 - val_loss: 0.0584 - val_acc: 0.9890\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 164s 6us/step - loss: 0.0575 - acc: 0.9891 - val_loss: 0.0588 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20778bf588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train_T.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_T, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid_T, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed data - larger kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 5, 41)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5, 40)             8240      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 2, 40)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_5 (Separabl (None, 2, 40)             1840      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 40)             160       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 10,726\n",
      "Trainable params: 10,646\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 183s 7us/step - loss: 0.1053 - acc: 0.9792 - val_loss: 0.0615 - val_acc: 0.9888\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 196s 7us/step - loss: 0.0599 - acc: 0.9889 - val_loss: 0.0593 - val_acc: 0.9890\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 181s 6us/step - loss: 0.0589 - acc: 0.9890 - val_loss: 0.0591 - val_acc: 0.9890\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 196s 7us/step - loss: 0.0585 - acc: 0.9890 - val_loss: 0.0591 - val_acc: 0.9889\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 184s 7us/step - loss: 0.0582 - acc: 0.9891 - val_loss: 0.0597 - val_acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20546ca550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train_T.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=5, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=5, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_T, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid_T, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed data - more larger kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 5, 41)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 5, 40)             11520     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 2, 40)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_6 (Separabl (None, 2, 40)             1920      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 40)             160       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 14,086\n",
      "Trainable params: 14,006\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/5\n",
      "27981697/27981697 [==============================] - 186s 7us/step - loss: 0.1022 - acc: 0.9805 - val_loss: 0.0641 - val_acc: 0.9885\n",
      "Epoch 2/5\n",
      "27981697/27981697 [==============================] - 181s 6us/step - loss: 0.0604 - acc: 0.9887 - val_loss: 0.0602 - val_acc: 0.9889\n",
      "Epoch 3/5\n",
      "27981697/27981697 [==============================] - 171s 6us/step - loss: 0.0592 - acc: 0.9890 - val_loss: 0.0593 - val_acc: 0.9891\n",
      "Epoch 4/5\n",
      "27981697/27981697 [==============================] - 183s 7us/step - loss: 0.0587 - acc: 0.9891 - val_loss: 0.0585 - val_acc: 0.9892\n",
      "Epoch 5/5\n",
      "27981697/27981697 [==============================] - 185s 7us/step - loss: 0.0585 - acc: 0.9891 - val_loss: 0.0588 - val_acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f140c081550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "\n",
    "input_shape = X_train_T.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=7, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=7, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train_T, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid_T, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from abc import abstractmethod\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "class Metric(Callback):\n",
    "    def __init__(self, model, X, y):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y_true = np.argmax(y, axis=1)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self._print_start_info()\n",
    "        y_pred = np.asarray(self.model.predict(self.X))\n",
    "        print('y_true', self.y_true.shape)\n",
    "        print('y_pred shape: ', y_pred.shape)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        print('y_pred shape after: ', y_pred.shape)\n",
    "        score = self._calc_metric(y_pred)\n",
    "        self._print_metric_score(score)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _print_start_info(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _calc_metric(self, y_pred):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _print_metric_score(self, score):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Precision(Metric):\n",
    "    def __init__(self, model, X, y, mode):\n",
    "        Metric.__init__(self, model, X, y)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _print_start_info(self):\n",
    "        print('Calculating Precision-{} ...'.format(self.mode))\n",
    "\n",
    "    def _calc_metric(self, y_pred):\n",
    "        return precision_score(self.y_true, y_pred, average=self.mode)\n",
    "\n",
    "    def _print_metric_score(self, score):\n",
    "        print('Precision: {:.4f}, mode: {}'.format(score, self.mode))\n",
    "\n",
    "\n",
    "class Recall(Metric):\n",
    "    def __init__(self, model, X, y, mode):\n",
    "        Metric.__init__(self, model, X, y)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _print_start_info(self):\n",
    "        print('Calculating Recall-{} ...'.format(self.mode))\n",
    "\n",
    "    def _calc_metric(self, y_pred):\n",
    "        return recall_score(self.y_true, y_pred, average=self.mode)\n",
    "\n",
    "    def _print_metric_score(self, score):\n",
    "        print('Recal: {:.4f}, mode: {}'.format(score, self.mode))\n",
    "\n",
    "\n",
    "class F1(Metric):\n",
    "    def __init__(self, model, X, y, mode):\n",
    "        Metric.__init__(self, model, X, y)\n",
    "        self.mode = mode\n",
    "\n",
    "    def _print_start_info(self):\n",
    "        print('Calculating F1-{} ...'.format(self.mode))\n",
    "\n",
    "    def _calc_metric(self, y_pred):\n",
    "        return f1_score(self.y_true, y_pred, average=self.mode)\n",
    "\n",
    "    def _print_metric_score(self, score):\n",
    "        print('F1: {:.4f}, mode: {}'.format(score, self.mode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric testing - micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_6 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 7,366\n",
      "Trainable params: 7,286\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/2\n",
      "27981697/27981697 [==============================] - 257s 9us/step - loss: 0.1001 - acc: 0.9794 - val_loss: 0.0599 - val_acc: 0.9888\n",
      "Epoch 2/2\n",
      "27981697/27981697 [==============================] - 258s 9us/step - loss: 0.0587 - acc: 0.9887 - val_loss: 0.0585 - val_acc: 0.9887\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "Precision: 0.9887, mode: micro\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "Recal: 0.9887, mode: micro\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "F1: 0.9887, mode: micro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd92a160908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 2\n",
    "\n",
    "metrics = [\n",
    "    Precision(model, X_valid, y_valid, mode='micro'),\n",
    "    Recall(model, X_valid, y_valid, mode='micro'),\n",
    "    F1(model, X_valid, y_valid, mode='micro')\n",
    "]\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "callbacks += metrics\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric testing - macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 41, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 41, 40)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_7 (Separabl (None, 20, 40)            1760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 4806      \n",
      "=================================================================\n",
      "Total params: 7,366\n",
      "Trainable params: 7,286\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27981697 samples, validate on 3109078 samples\n",
      "Epoch 1/2\n",
      "27981697/27981697 [==============================] - 289s 10us/step - loss: 0.1003 - acc: 0.9793 - val_loss: 0.0609 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "27981697/27981697 [==============================] - 270s 10us/step - loss: 0.0592 - acc: 0.9886 - val_loss: 0.0582 - val_acc: 0.9889\n",
      "Calculating Precision-macro ...\n",
      "y_true (3109078,)\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "Precision: 0.9139, mode: macro\n",
      "Calculating Recall-macro ...\n",
      "y_true (3109078,)\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "Recal: 0.8496, mode: macro\n",
      "Calculating F1-macro ...\n",
      "y_true (3109078,)\n",
      "y_pred shape:  (3109078, 6)\n",
      "y_pred shape after:  (3109078,)\n",
      "F1: 0.8633, mode: macro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd92a1747b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            return lr * 0.95\n",
    "    return lr\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "num_output_classes = y_train.shape[1]\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_1 = Conv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(input_layer)\n",
    "pool_1 = MaxPooling1D(pool_size=(2))(conv_1)\n",
    "conv_2 = SeparableConv1D(filters=40, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l2(0.01))(pool_1)\n",
    "bn_1 = BatchNormalization()(conv_2)\n",
    "\n",
    "flatten = Flatten()(bn_1)\n",
    "predictions = Dense(num_output_classes, activation='softmax')(flatten)\n",
    "\n",
    "model = Model(input_layer, predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 2\n",
    "\n",
    "metrics = [\n",
    "    Precision(model, X_valid, y_valid, mode='macro'),\n",
    "    Recall(model, X_valid, y_valid, mode='macro'),\n",
    "    F1(model, X_valid, y_valid, mode='macro')\n",
    "]\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "callbacks = [lr_callback, EarlyStopping(monitor='val_loss', patience=3)]\n",
    "callbacks += metrics\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
