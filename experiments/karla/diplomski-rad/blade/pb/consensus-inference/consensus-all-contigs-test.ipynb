{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "module_path = '/home/diplomski-rad/tmp/consensus-net/src/python/dataset/'\n",
    "if module_path not in sys.path:\n",
    "    print('Adding dataset module.')\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import progressbar\n",
    "import pysam\n",
    "import pysamstats\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Bio import SeqIO\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "CONSENSUS_SUMMARY_CMD_1 = '{}/mummer3.23/dnadiff -p {}/dnadiff-output {} {} ' \\\n",
    "                          '2>> {}/err'\n",
    "CONSENSUS_SUMMARY_CMD_2 = 'head -n 24 {}/dnadiff-output.report | tail -n 20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pileups code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_pileups(bam_file_path, reference_fasta_path, include_indels=True):\n",
    "    \"\"\"\n",
    "    Generate pileups from reads alignment to reference.\n",
    "\n",
    "    Pileups are generated for all contigs.\n",
    "\n",
    "    :param bam_file_path: path to .bam file containing alignments\n",
    "    :type bam_file_path: str\n",
    "    :param reference_fasta_path: path to .fasta file\n",
    "    :type reference_fasta_path: str\n",
    "    :param include_indels: flag which indicates whether to include indels in\n",
    "        pileups\n",
    "    :type include_indels: bool\n",
    "    :return: pileups (X)\n",
    "    :rtype: tuple of np.ndarray and list of str\n",
    "    \"\"\"\n",
    "    bam_file = pysam.AlignmentFile(bam_file_path)\n",
    "\n",
    "    if include_indels:\n",
    "        info_of_interest = ['A', 'C', 'G', 'T', 'insertions', 'deletions']\n",
    "    else:\n",
    "        info_of_interest = ['A', 'C', 'G', 'T']\n",
    "\n",
    "    pileups = [np.zeros(\n",
    "        (bam_file.get_reference_length(contig_name),\n",
    "         len(info_of_interest)\n",
    "         )) for contig_name in bam_file.references]\n",
    "\n",
    "    total_length = np.sum(\n",
    "        [bam_file.get_reference_length(contig_name) for contig_name in\n",
    "         bam_file.references])\n",
    "    progress_counter = 0\n",
    "    with progressbar.ProgressBar(max_value=total_length) as progress_bar:\n",
    "        for contig_id, contig_name in enumerate(bam_file.references):\n",
    "            for record in pysamstats.stat_variation(\n",
    "                    bam_file, chrom=contig_name, fafile=reference_fasta_path):\n",
    "                progress_bar.update(progress_counter)\n",
    "                progress_counter += 1\n",
    "                for i, info in enumerate(info_of_interest):\n",
    "                    pileups[contig_id][record['pos']][i] += record[info]\n",
    "\n",
    "    return pileups, bam_file.references\n",
    "\n",
    "\n",
    "def _generate_ground_truth(reference_fasta_path, ordered_contigs):\n",
    "    \"\"\"\n",
    "    Generates ground truth - nucleus bases from reference.\n",
    "\n",
    "    It parses all contigs in same order as when generating pileups to make\n",
    "    sure that every X corresponds to correct y.\n",
    "\n",
    "    :param reference_fasta_path: path to .fasta file\n",
    "    :type reference_fasta_path: str\n",
    "    :param ordered_contigs: list of contigs\n",
    "    :type ordered_contigs: list of str\n",
    "    :return: nucleus bases from reference (y)\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    record_dict = SeqIO.to_dict(SeqIO.parse(reference_fasta_path, 'fasta'))\n",
    "    total_options = 5\n",
    "    y_oh = [np.zeros((len(record_dict[contig_name]), total_options)) for\n",
    "            contig_name in ordered_contigs]\n",
    "    # Last number in shape - 5 - is for letters other than A, C, G and T.\n",
    "    mapping = {'A': 0, 'a': 0, 'C': 1, 'c': 1, 'G': 2, 'g': 2, 'T': 3, 't': 3}\n",
    "\n",
    "    total_length = np.sum(\n",
    "        len(record_dict[contig_name]) for contig_name in ordered_contigs)\n",
    "    progress_counter = 0\n",
    "    with progressbar.ProgressBar(max_value=total_length) as progress_bar:\n",
    "        for contig_id, contig_name in enumerate(ordered_contigs):\n",
    "            contig = record_dict[contig_name]\n",
    "            print('Parsing contig {}, len: {}'.format(contig_name, len(contig)))\n",
    "            for position, base in enumerate(contig.seq):\n",
    "                progress_bar.update(progress_counter)\n",
    "                progress_counter += 1\n",
    "                y_oh[contig_id][position][mapping.get(base, -1)] = 1\n",
    "    return y_oh\n",
    "\n",
    "\n",
    "def generate_pileups(bam_file_path, reference_fasta_path, mode,\n",
    "                     save_directory_path=None, include_indels=True):\n",
    "    \"\"\"\n",
    "    Generates pileups from given alignment stored in bam file.\n",
    "\n",
    "    Mode must be one of 'training' or 'inference' string indicating pileups\n",
    "    generation mode. If 'training' is selected, pileups from different\n",
    "    contigs are all be concatenated. If 'inference' is selected, pileups from\n",
    "    different contig will be hold separate to enable to make consensus with\n",
    "    same number of contigs.\n",
    "\n",
    "    If save_directory_path is provided, generated pileups are stored in that\n",
    "    directory.\n",
    "\n",
    "    If include_indels is set to True, indels will also we included in\n",
    "    pileups. Otherwise, only nucleus bases will be in pileup (A, C, G and T).\n",
    "\n",
    "    :param bam_file_path: path to .bam file containing alignments\n",
    "    :type bam_file_path: str\n",
    "    :param reference_fasta_path: path to .fasta file\n",
    "    :type reference_fasta_path: str\n",
    "    :param mode: either 'training' or 'inference' string, representing the\n",
    "        mode for pileups generation\n",
    "    :type mode: str\n",
    "    :param save_directory_path: path to directory for storing pileups\n",
    "    :type save_directory_path: str\n",
    "    :param include_indels: flag which indicates whether to include indels in\n",
    "        pileups\n",
    "    :type include_indels: bool\n",
    "    :return: pileups (X) and matching nucleus bases from reference (y) with\n",
    "        concatenated contigs for inference mode, or separate contigs for\n",
    "        training mode; also, if save_directory_path is provided, list of\n",
    "        paths to saved files is returned in tuple (X, y_oh, X_save_paths,\n",
    "        y_save_paths). In both cases list of contig names is also returned.\n",
    "    :rtype tuple of np.ndarray and list of str or tuple of np.array and list of str\n",
    "    \"\"\"\n",
    "    _check_mode(mode)\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        if os.path.exists(save_directory_path):\n",
    "            raise ValueError('You must provide non-existing save output '\n",
    "                             'directory, {} given.'.format(save_directory_path))\n",
    "        else:\n",
    "            os.makedirs(save_directory_path)\n",
    "\n",
    "    print('##### Generate pileups from read alignments to reference. #####')\n",
    "\n",
    "    print('-----> 1. Generate pileups. <-----')\n",
    "    X, ordered_contigs = _generate_pileups(bam_file_path,\n",
    "                                           reference_fasta_path,\n",
    "                                           include_indels=include_indels)\n",
    "\n",
    "    print('-----> 2. Generate ground truth. <-----')\n",
    "    y_oh = _generate_ground_truth(reference_fasta_path, ordered_contigs)\n",
    "\n",
    "    total_pileups = len(X)\n",
    "    if mode == 'training': # training mode\n",
    "        X, y_oh = np.concatenate(X, axis=0), np.concatenate(y_oh, axis=0)\n",
    "        total_pileups = 1\n",
    "    else:  # inference mode\n",
    "        pass  # nothing to do\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        X_save_paths = [\n",
    "            os.path.join(\n",
    "                save_directory_path,\n",
    "                'pileups-X-{}{}.npy'.format(\n",
    "                    i, '-indels' if include_indels else ''))\n",
    "            for i in range(total_pileups)]\n",
    "        y_save_paths = [os.path.join(save_directory_path,\n",
    "                        'pileups-y-{}.npy'.format(i))\n",
    "                        for i in range(total_pileups)]\n",
    "        for X_save_path, y_save_path, Xi, yi in zip(\n",
    "                X_save_paths, y_save_paths, X, y_oh):\n",
    "            np.save(X_save_path, Xi)\n",
    "            np.save(y_save_path, yi)\n",
    "        return X, y_oh, X_save_paths, y_save_paths, ordered_contigs\n",
    "\n",
    "    return X, y_oh, ordered_contigs\n",
    "\n",
    "\n",
    "def _check_mode(mode):\n",
    "    \"\"\"\n",
    "    Checks if given mode is supported: 'training' or 'inference'.\n",
    "\n",
    "    :param mode: mode to check\n",
    "    :type mode: str\n",
    "    :raise ValueError: if selected mode is not supported\n",
    "    \"\"\"\n",
    "    modes = ['training', 'inference']\n",
    "    if mode not in modes:\n",
    "        raise ValueError('You must provide either \\'training\\' or '\n",
    "                         '\\'inference\\' mode, but \\'{}\\' given.'.format(mode))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_empty_rows(X):\n",
    "    \"\"\"\n",
    "    Calculates which rows in X are empty rows (i.e. all numbers in that row\n",
    "    are equal to 0).\n",
    "\n",
    "    :param X: 2-D data\n",
    "    :type X: np.ndarray\n",
    "    :return: 1-D array with 1s on positions which correspond to empty rows.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    empty_row = np.zeros((1, X.shape[1]))  # size is second axis of X\n",
    "    empty_rows = [int(v) for v in np.all(empty_row == X, axis=1)]\n",
    "    return empty_rows\n",
    "\n",
    "\n",
    "def create_dataset_with_neighbourhood(X_paths, y_paths, neighbourhood_size,\n",
    "                                      mode, save_directory_path=None):\n",
    "    \"\"\"\n",
    "    Creates datasets by mixing all pileups with given neighbourhood_size.\n",
    "\n",
    "    Datasets are concatenated after extracting neighbourhood_size positions in\n",
    "    given datasets separately if 'training' mode is selected. If 'inference'\n",
    "    mode is selected, X_paths are assumed to be paths to different contigs of\n",
    "    same\n",
    "\n",
    "    Dataset at i-th position in X_paths should match given labels at i-th\n",
    "    position in y_paths.\n",
    "\n",
    "    If save_directory_path is provided, generated pileups are stored in that\n",
    "    directory.\n",
    "\n",
    "    :param X_paths: list of paths to X pileup dataset\n",
    "    :type X_paths: list of str\n",
    "    :param y_paths: list of paths to y pileup dataset\n",
    "    :type y_paths: list of str\n",
    "    :param neighbourhood_size: number of neighbours to use from one size (eg.\n",
    "        if you set this parameter to 3, it will take 3 neighbours from both\n",
    "        sides so total number of positions in one example will be 7 -\n",
    "        counting the middle position)\n",
    "    :type neighbourhood_size: int\n",
    "    :param mode: either 'training' or 'inference' string, representing the\n",
    "        mode for pileups generation\n",
    "    :type mode: str\n",
    "    :param save_directory_path: path to directory for storing dataset\n",
    "    :type save_directory_path: str\n",
    "    :return:\n",
    "    :rtype tuple of np.ndarray or tuple of np.array and list of str\n",
    "    \"\"\"\n",
    "    _check_mode(mode)\n",
    "\n",
    "    if not len(X_paths) == len(y_paths):\n",
    "        raise ValueError('Number of X_paths and y_paths should be the same!')\n",
    "\n",
    "    # If training mode is selected, all pileups will be concatenated.\n",
    "    total_pileups = 1 if mode == 'training' else len(X_paths)\n",
    "\n",
    "    X_save_paths, y_save_paths = None, None\n",
    "    if save_directory_path is not None:\n",
    "        X_save_paths, y_save_paths = _generate_save_paths(neighbourhood_size,\n",
    "                                                          save_directory_path,\n",
    "                                                          total_pileups)\n",
    "\n",
    "    X_neighbourhood_list, y_neighbourhood_list = list(), list()\n",
    "    for X_path, y_path in zip(X_paths, y_paths):\n",
    "        print('Parsing ', X_path, ' and ', y_path)\n",
    "\n",
    "        curr_X, curr_y = np.load(X_path), np.load(y_path)\n",
    "        # Removing last column which contains everything which was not 'A' nor\n",
    "        # 'C' nor 'G' nor 'T'.\n",
    "        curr_y = curr_y[:, :4]\n",
    "        new_curr_X, new_curr_y = list(), list()\n",
    "        empty_rows = _calc_empty_rows(curr_X)\n",
    "\n",
    "        print('Creating dataset with neighbourhood ...')\n",
    "        with progressbar.ProgressBar(max_value=curr_X.shape[0]) as progress_bar:\n",
    "            # TODO(ajuric): Check if this can be speed up.\n",
    "            for i in range(curr_X.shape[0]):\n",
    "                progress_bar.update(i)\n",
    "                if empty_rows[i] == 1:\n",
    "                    continue  # current row is empty row\n",
    "                if i < neighbourhood_size or \\\n",
    "                   i >= curr_X.shape[0] - neighbourhood_size:\n",
    "                    # Current position is not suitable to build an example.\n",
    "                    continue\n",
    "                zeros_to_left = np.sum(empty_rows[i - neighbourhood_size:i])\n",
    "                zeros_to_right = np.sum(\n",
    "                    empty_rows[i + 1:i + neighbourhood_size + 1])\n",
    "                if zeros_to_left == 0 and zeros_to_right == 0:\n",
    "                    new_curr_X.append(\n",
    "                        curr_X[\n",
    "                            i - neighbourhood_size:\n",
    "                            i + neighbourhood_size + 1])\n",
    "                    new_curr_y.append(curr_y[i])\n",
    "\n",
    "        X_neighbourhood_list.append(np.array(new_curr_X))\n",
    "        y_neighbourhood_list.append(np.array(new_curr_y))\n",
    "\n",
    "    if mode == 'training':\n",
    "        X_neighbourhood_list = [np.concatenate(X_neighbourhood_list, axis=0)]\n",
    "        y_neighbourhood_list = [np.concatenate(y_neighbourhood_list, axis=0)]\n",
    "    else:  # inference mode\n",
    "        pass  # nothing to do\n",
    "\n",
    "    if save_directory_path is not None:\n",
    "        for X_save_path, y_save_path, Xi, yi in zip(\n",
    "            X_save_paths, y_save_paths, X_neighbourhood_list,\n",
    "                y_neighbourhood_list):\n",
    "            np.save(X_save_path, Xi)\n",
    "            np.save(y_save_path, yi)\n",
    "        return X_neighbourhood_list, \\\n",
    "               y_neighbourhood_list, \\\n",
    "               X_save_paths, \\\n",
    "               y_save_paths\n",
    "\n",
    "    return X_neighbourhood_list, y_neighbourhood_list\n",
    "\n",
    "\n",
    "def _generate_save_paths(neighbourhood_size, save_directory_path,\n",
    "                         total_pileups):\n",
    "    \"\"\"\n",
    "    Generates a list of save paths for dataset X and y.\n",
    "\n",
    "    :param neighbourhood_size: number of neighbours to use from one size (eg.\n",
    "        if you set this parameter to 3, it will take 3 neighbours from both\n",
    "        sides so total number of positions in one example will be 7 -\n",
    "        counting the middle position)\n",
    "    :type neighbourhood_size: int\n",
    "    :param save_directory_path: path to directory for storing dataset\n",
    "    :type save_directory_path: str\n",
    "    :param total_pileups: total number of pileups to be generated a the end;\n",
    "        determines the number of save paths to be generated\n",
    "    :type total_pileups: int\n",
    "    :return: tuple of list of str\n",
    "    \"\"\"\n",
    "    # Creating save file paths.\n",
    "    X_save_paths = [os.path.join(\n",
    "        save_directory_path,\n",
    "        'X-pileups-n{}-{}.npy'.format(neighbourhood_size, i))\n",
    "                    for i in range(total_pileups)]\n",
    "    y_save_paths = [os.path.join(\n",
    "        save_directory_path,\n",
    "        'y-pileups-n{}-{}.npy'.format(neighbourhood_size, i))\n",
    "                    for i in range(total_pileups)]\n",
    "\n",
    "    for X_save_path, y_save_path in zip(X_save_paths, y_save_paths):\n",
    "        if os.path.exists(X_save_path) or os.path.exists(y_save_path):\n",
    "            raise ValueError('Pileups already exists in given save '\n",
    "                             'directory. Either provide other save '\n",
    "                             'directory or empty this one.')\n",
    "    return X_save_paths, y_save_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_and_reshape_for_conv(X_paths, y_paths, validation_size=None):\n",
    "    \"\"\"\n",
    "    Reads X and y from given paths and reshapes them for applying in\n",
    "    convolutional networks.\n",
    "\n",
    "    Reshaping is done by splitting different letters in separate channels,\n",
    "    eg. letter 'A' has it's own channel, letter 'C' has it's own channel, etc.\n",
    "\n",
    "    :param path_X: list of paths to X data\n",
    "    :type path_X: list of str\n",
    "    :param path_y: list of paths to y data\n",
    "    :type path_y: list of str\n",
    "    :param validation_size: specifies percentage of dataset used for validation\n",
    "    :type validation_size: float\n",
    "    :return: If validation_size is None, returns just X and y reshaped. If\n",
    "    validation_size is float, returns a tuple in following order: (X, y,\n",
    "    X_train, X_validate, y_train, y_validate).\n",
    "    :rtype: tuple of np.ndarray\n",
    "    \"\"\"\n",
    "    if not len(X_paths) == len(y_paths):\n",
    "        raise ValueError('Number of X_paths and y_paths must be the same!')\n",
    "    \n",
    "    if validation_size is not None:\n",
    "        if validation_size < 0 or validation_size > 1.0:\n",
    "            raise ValueError('Validation size must be float from [0, 1], but {}'\n",
    "                             ' given.'.format(validation_size))\n",
    "        if not len(path_X) == 1:\n",
    "            raise ValueError('Validation size can only be provided if there is only one X_path and y_path.')\n",
    "    \n",
    "    X_list, y_list = list(), list()\n",
    "    for X_path, y_path in zip(X_paths, y_paths):\n",
    "        X, y = np.load(X_path), np.load(y_path)\n",
    "        print('X shape before reshaping:', X.shape)\n",
    "        print('y shape before reshaping:', y.shape)\n",
    "\n",
    "        new_X = list()\n",
    "        neighbourhood_size = X[0].shape[0]\n",
    "        # Number of columns is equal to the number of letters in dataset (A, C,\n",
    "        # G, T, I, D, ...).\n",
    "        num_columns = X[0].shape[1]\n",
    "        num_data = X.shape[0]\n",
    "        with progressbar.ProgressBar(max_value=num_data) as progress_bar:\n",
    "            for i, xi in enumerate(X):\n",
    "                new_xi = np.dstack(\n",
    "                    [xi[:, col_index].reshape(neighbourhood_size, 1)\n",
    "                     for col_index in range(num_columns)]\n",
    "                )\n",
    "                new_X.append(new_xi)\n",
    "                progress_bar.update(i)\n",
    "\n",
    "        new_X = np.array(new_X)\n",
    "        X = new_X\n",
    "        print('X shape after reshaping:', X.shape)\n",
    "        print('y shape after reshaping:', y.shape)\n",
    "        \n",
    "        X_list.append(X), y_list.append(y)\n",
    "\n",
    "    if validation_size is None:\n",
    "        return X_list, y_list\n",
    "    else:\n",
    "        # There is only one X and y (because, all datasets are concatenated for training).\n",
    "        X, y = X_list[0], y_list[0]\n",
    "        print('Splitting to train and validation set.')\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "            X, y, test_size=validation_size)\n",
    "        print('X_train shape:', X_train.shape)\n",
    "        print('X_validate shape:', X_validate.shape)\n",
    "        print('y_train:', y_train.shape)\n",
    "        print('y_validate:', y_validate.shape)\n",
    "        return X, y, X_train, X_validate, y_train, y_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_predictions_to_genome(predictions):\n",
    "    mapping = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
    "    genome = [mapping[prediction] for prediction in predictions]\n",
    "    return genome\n",
    "\n",
    "\n",
    "def _write_genome_to_fasta(contigs, fasta_file_path, contig_names):\n",
    "    with open(fasta_file_path, 'w') as f:\n",
    "        for contig, contig_name in zip(contigs, contig_names):\n",
    "            f.write('>{} LN:{}\\n'.format(contig_name, len(contig)))\n",
    "            f.write('{}\\n'.format(''.join(contig)))\n",
    "\n",
    "def make_consensus(model_path, assembly_fasta_path, reference_path,\n",
    "                   bam_file_path, neighbourhood_size, output_dir,\n",
    "                   tools_dir, include_indels=True):\n",
    "    \n",
    "    # TODO(ajuric): Currently, y is also created while calculating consensus, due to\n",
    "    # reuising existing code from training. But, here in inference y is not used.\n",
    "    # This needs to be removed to reduce the unnecessary overhead.\n",
    "        \n",
    "    print('----> Create pileups from assembly. <----')\n",
    "    X, y, X_save_paths, y_save_paths, contig_names = generate_pileups(bam_file_path,\n",
    "                                    assembly_fasta_path,\n",
    "                                    mode='inference',\n",
    "                                    save_directory_path=output_dir,\n",
    "                                    include_indels=include_indels)\n",
    "\n",
    "    print('----> Create dataset with neighbourhood from pileups. <----')\n",
    "    X, y, X_save_paths, y_save_paths = create_dataset_with_neighbourhood(X_save_paths,\n",
    "                                                     y_save_paths,\n",
    "                                                     neighbourhood_size,\n",
    "                                                     mode='inference',\n",
    "                                                     save_directory_path=output_dir)\n",
    "\n",
    "    print('----> Reshape dataset for convolutional network. <----')\n",
    "    X_list, y_list = read_dataset_and_reshape_for_conv(X_save_paths, y_save_paths)\n",
    "\n",
    "    print('----> Load model and make predictions (consensus). <----')\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    contigs = list()\n",
    "    for X, y, contig_name in zip(X_list, y_list, contig_names):\n",
    "        probabilities = model.predict(X)\n",
    "        predictions = np.argmax(probabilities, axis=1)\n",
    "\n",
    "        contig = _convert_predictions_to_genome(predictions)\n",
    "        contigs.append(contig)\n",
    "        \n",
    "    consensus_path = os.path.join(output_dir, 'consensus.fasta')\n",
    "    _write_genome_to_fasta(contigs, consensus_path, contig_names)\n",
    "\n",
    "    print('----> Create consensus summary. <----')\n",
    "    os.system(CONSENSUS_SUMMARY_CMD_1.format(tools_dir, output_dir,\n",
    "                                             reference_path,\n",
    "                                             consensus_path, output_dir))\n",
    "    os.system(CONSENSUS_SUMMARY_CMD_2.format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/diplomski-rad/blade/pb/datasets/n3-all/model.h5'\n",
    "assembly_fasta_path = '/home/diplomski-rad/blade/pb/morganela-morgani-NCTC235/iter2.fasta'\n",
    "reference_path = '/home/data/pacific_biosciences/bacteria/morganella/morgani/morganella_morganii_reference.fasta'\n",
    "bam_file_path = '/home/diplomski-rad/blade/pb/morganela-morgani-NCTC235/reads-to-asm-sorted.bam'\n",
    "neighbourhood_size = 3\n",
    "output_dir = './consensus-all-contigs-test-directory'\n",
    "tools_dir = '/home/diplomski-rad/'\n",
    "include_indels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1514 of 3820508) |                 | Elapsed Time: 0:00:00 ETA:   0:04:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Create pileups from assembly. <----\n",
      "##### Generate pileups from read alignments to reference. #####\n",
      "-----> 1. Generate pileups. <-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (3820508 of 3820508) |##############| Elapsed Time: 0:04:50 Time:  0:04:50\n",
      "N/A% (0 of 3820508) |                    | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> 2. Generate ground truth. <-----\n",
      "Parsing contig utg000001l, len: 2183309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57% (2212884 of 3820508) |########      | Elapsed Time: 0:00:11 ETA:   0:00:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing contig utg000002l, len: 1637199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (3820508 of 3820508) |##############| Elapsed Time: 0:00:20 Time:  0:00:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Create dataset with neighbourhood from pileups. <----\n",
      "Parsing  ./consensus-all-contigs-test-directory/pileups-X-0.npy  and  ./consensus-all-contigs-test-directory/pileups-y-0.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (4218 of 2183309) |                 | Elapsed Time: 0:00:00 ETA:   0:01:43"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with neighbourhood ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2183309 of 2183309) |##############| Elapsed Time: 0:01:46 Time:  0:01:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing  ./consensus-all-contigs-test-directory/pileups-X-1.npy  and  ./consensus-all-contigs-test-directory/pileups-y-1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (4236 of 1637199) |                 | Elapsed Time: 0:00:00 ETA:   0:01:17"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with neighbourhood ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1637199 of 1637199) |##############| Elapsed Time: 0:01:18 Time:  0:01:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Reshape dataset for convolutional network. <----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (2998 of 2183241) |                 | Elapsed Time: 0:00:00 ETA:   0:01:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before reshaping: (2183241, 7, 4)\n",
      "y shape before reshaping: (2183241, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2183241 of 2183241) |##############| Elapsed Time: 0:01:16 Time:  0:01:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after reshaping: (2183241, 7, 1, 4)\n",
      "y shape after reshaping: (2183241, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (5804 of 1637112) |                 | Elapsed Time: 0:00:00 ETA:   0:00:56"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before reshaping: (1637112, 7, 4)\n",
      "y shape before reshaping: (1637112, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1637112 of 1637112) |##############| Elapsed Time: 0:00:57 Time:  0:00:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after reshaping: (1637112, 7, 1, 4)\n",
      "y shape after reshaping: (1637112, 4)\n",
      "----> Load model and make predictions (consensus). <----\n",
      "----> Create consensus summary. <----\n"
     ]
    }
   ],
   "source": [
    "make_consensus(\n",
    "    model_path,\n",
    "    assembly_fasta_path,\n",
    "    reference_path,\n",
    "    bam_file_path,\n",
    "    neighbourhood_size,\n",
    "    output_dir,\n",
    "    tools_dir,\n",
    "    include_indels=include_indels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
